{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"V28","collapsed_sections":["3AB4HgugYF4p","lPB56EIxYHaW","J2B-82ILYI4o","yA38gqvvQmLn","ChXU-_xITHBR"],"machine_shape":"hm","authorship_tag":"ABX9TyNjIlxA6Uc3IhMYjBkmMDIH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["# ===== Mount Google Drive =====\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GrnczAhZg6yw","executionInfo":{"status":"ok","timestamp":1756833390177,"user_tz":-60,"elapsed":15649,"user":{"displayName":"Peerasit Srisukontamit","userId":"15163058641388219245"}},"outputId":"54b3bb77-0f7b-410e-b244-6201aa81e4aa"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# WITH CONTEXT"],"metadata":{"id":"xSSLWNMkYB4C"}},{"cell_type":"markdown","source":["## Dataset Generation"],"metadata":{"id":"3AB4HgugYF4p"}},{"cell_type":"code","source":["# ===== Imports =====\n","import os\n","import math\n","import random\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","# ===== Paths =====\n","INPUT_CSV = \"/content/drive/MyDrive/Colab Notebooks/Flanker-GPT/trialdata74.csv\"\n","OUT_DIR   = \"/content/drive/MyDrive/Colab Notebooks/Flanker-GPT/human_dataset\"\n","os.makedirs(OUT_DIR, exist_ok=True)\n","\n","# ===== Config =====\n","RNG_SEED = 42 # reproducibility\n","VAL_RATIO = 0.10 # 10% validation\n","WINDOW_SIZE = 4 # seq length for context window\n","NO_CONTEXT = False # True -> trial 4 only (no-context baseline)\n","MAX_SESSIONS = 20000 # limit number of sessions for speed; set None for all\n","KEEP_RT_COL = True # export human RT into meta for later calibration\n","DROP_NAN_RT_META = False # if True, drop rows with NaN RT from meta (sequence still saved)\n","\n","# ===== Tokens (4 directions + '0' + '=>') =====\n","TOKENS = ['R', 'U', 'L', 'D', '0', '=>']  # NOTE: label is response_direction ∈ {R,U,L,D}\n","stoi = {tok: i for i, tok in enumerate(TOKENS)}\n","itos = {i: tok for tok, i in stoi.items()}\n","\n","# ===== Layout mapping (layout_id → 5 positions in 5x5) =====\n","LAYOUTS = {\n","    0: [  2,  7, 12, 17, 22 ], # Vertical\n","    1: [ 10, 11, 12, 13, 14 ], # Horizontal\n","    2: [  4,  7, 10, 17, 24 ], # Left hook\n","    3: [  0,  7, 14, 17, 20 ], # Right hook\n","    4: [ 11, 13,  2, 20, 24 ], # Top hook\n","    5: [  0,  4, 22, 11, 13 ], # Bottom hook\n","    6: [ 10,  2, 12, 14, 22 ], # Surround\n","}\n","\n","# ===== Helpers =====\n","def build_matrix(layout_id, target_dir, flanker_dir):\n","    \"\"\"Build a 5x5 matrix (flattened to length 25) for one trial.\"\"\"\n","    mat = ['0'] * 25\n","    pos = LAYOUTS[int(layout_id)]\n","    # pos[2] is target; others are flankers\n","    mat[pos[0]] = flanker_dir\n","    mat[pos[1]] = flanker_dir\n","    mat[pos[2]] = target_dir\n","    mat[pos[3]] = flanker_dir\n","    mat[pos[4]] = flanker_dir\n","    return mat\n","\n","def encode_sequence(trials_tokens, label_token):\n","    \"\"\"Flatten trials (each 25 tokens) + '=>' + label into token ids.\"\"\"\n","    seq = []\n","    for t in trials_tokens:\n","        seq.extend(t)\n","    seq.append('=>')\n","    seq.append(label_token)\n","    return [stoi[t] for t in seq]\n","\n","def is_congruent_row(target_dir, flanker_dir):\n","    return str(target_dir) == str(flanker_dir)\n","\n","def valid_dir(tok):\n","    return str(tok) in {'R','U','L','D'}\n","\n","# ===== Load RAW CSV =====\n","dtype_map = {\n","    'game_result_id': 'int64',\n","    'trial_num': 'int64',\n","    'target_direction': 'string',\n","    'flanker_direction': 'string',\n","    'stimulus_layout': 'int64',\n","    'response_direction': 'string',\n","    'correct': 'string',\n","    'response_time': 'float64',\n","    'user_id': 'int64',\n","}\n","print(\"Loading raw CSV ...\")\n","df = pd.read_csv(INPUT_CSV, dtype=dtype_map)\n","\n","# keep only complete direction rows\n","df = df[ df['target_direction'].apply(valid_dir)\n","       & df['flanker_direction'].apply(valid_dir)\n","       & df['response_direction'].apply(valid_dir) ].copy()\n","\n","# sort by user/session/trial\n","df = df.sort_values(by=['user_id', 'game_result_id', 'trial_num']).reset_index(drop=True)\n","print(f\"✅ Loaded rows after filtering: {len(df):,}\")\n","\n","# ===== Build sequences =====\n","random.seed(RNG_SEED)\n","np.random.seed(RNG_SEED)\n","\n","encoded_sequences = []\n","meta_rows = []\n","\n","grouped = df.groupby(['game_result_id'], sort=False)\n","total_sessions = df['game_result_id'].nunique()\n","limit_sessions = min(MAX_SESSIONS, total_sessions) if MAX_SESSIONS is not None else total_sessions\n","\n","print(f\"Building sequences (WINDOW_SIZE={WINDOW_SIZE}, NO_CONTEXT={NO_CONTEXT}) ...\")\n","for i, (gid, g) in enumerate(tqdm(grouped, total=total_sessions, desc=\"Sessions\")):\n","    if i >= limit_sessions:\n","        break\n","\n","    rows = g.to_dict('records')\n","    if len(rows) < WINDOW_SIZE:\n","        continue\n","\n","    for end_idx in range(WINDOW_SIZE-1, len(rows)):\n","        start_idx = end_idx - (WINDOW_SIZE - 1)\n","        window_rows = rows[start_idx:end_idx+1]\n","        use_rows = [window_rows[-1]] if NO_CONTEXT else window_rows\n","\n","        trials_tokens = []\n","        skip = False\n","        for r in use_rows:\n","            try:\n","                mat = build_matrix(r['stimulus_layout'], r['target_direction'], r['flanker_direction'])\n","            except Exception as e:\n","                skip = True\n","                break\n","            trials_tokens.append(mat)\n","        if skip:\n","            continue\n","\n","        # label is the *human response* on trial 4\n","        label_token = str(window_rows[-1]['response_direction'])\n","        token_ids = encode_sequence(trials_tokens, label_token)\n","        encoded_sequences.append(token_ids)\n","\n","        # meta for trial 4 (end of window)\n","        t4 = window_rows[-1]\n","        rt_val = float(t4['response_time']) if pd.notna(t4['response_time']) else math.nan\n","\n","        meta_rows.append({\n","            \"user_id\":             int(t4['user_id']),\n","            \"game_result_id\":      int(t4['game_result_id']),\n","            \"end_trial_num\":       int(t4['trial_num']),\n","            \"context_used\":        int(not NO_CONTEXT),\n","            \"trial4_layout\":       int(t4['stimulus_layout']),\n","            \"trial4_target\":       str(t4['target_direction']),\n","            \"trial4_flanker\":      str(t4['flanker_direction']),\n","            \"trial4_is_congruent\": int(is_congruent_row(t4['target_direction'], t4['flanker_direction'])),\n","            \"trial4_response\":     str(t4['response_direction']),\n","            \"trial4_correct\":      1 if str(t4['correct']).upper() == 'T' else 0,\n","            \"trial4_response_time\": rt_val if KEEP_RT_COL else np.nan,\n","            # raw text (optional, helpful for debugging; keep or drop as needed)\n","            \"trial_1_4_text\": \" \".join([tok for trial in trials_tokens for tok in trial]),\n","            \"label_response_token_id\": int(stoi[label_token]),\n","        })\n","\n","print(f\"✅ Total sequences built: {len(encoded_sequences):,}\")\n","\n","# ===== Sanity: length alignment =====\n","assert len(encoded_sequences) == len(meta_rows), \"encoded_sequences and meta_rows length mismatch!\"\n","\n","# ===== Shuffle & Split with aligned meta =====\n","print(\"Shuffling and splitting with aligned meta ...\")\n","N = len(encoded_sequences)\n","perm = np.random.RandomState(RNG_SEED).permutation(N)\n","encoded_sequences = [encoded_sequences[i] for i in perm]\n","meta_rows        = [meta_rows[i]        for i in perm]\n","\n","split_idx = int(N * (1 - VAL_RATIO))\n","train_set = encoded_sequences[:split_idx]\n","val_set   = encoded_sequences[split_idx:]\n","\n","train_meta = meta_rows[:split_idx]\n","val_meta   = meta_rows[split_idx:]\n","\n","# (Optional) drop rows with NaN RT from meta ONLY (sequence remains; usually keep them)\n","if DROP_NAN_RT_META:\n","    def _drop_nan_rt(meta_list):\n","        kept = []\n","        for m in meta_list:\n","            rt = m.get(\"trial4_response_time\", np.nan)\n","            if not (isinstance(rt, float) and math.isnan(rt)):\n","                kept.append(m)\n","        return kept\n","    train_meta = _drop_nan_rt(train_meta)\n","    val_meta   = _drop_nan_rt(val_meta)\n","\n","# ===== Save .npy =====\n","np.save(os.path.join(OUT_DIR, 'train.npy'), np.array(train_set, dtype=np.int32))\n","np.save(os.path.join(OUT_DIR, 'val.npy'),   np.array(val_set,   dtype=np.int32))\n","print(f\"Saved .npy to: {OUT_DIR}\")\n","print(f\"train.npy: {len(train_set):,} | val.npy: {len(val_set):,}\")\n","\n","# ===== Save aligned meta CSV =====\n","train_meta_df = pd.DataFrame(train_meta)\n","val_meta_df   = pd.DataFrame(val_meta)\n","\n","train_meta_path = os.path.join(OUT_DIR, 'train_meta.csv')\n","val_meta_path   = os.path.join(OUT_DIR, 'val_meta.csv')\n","train_meta_df.to_csv(train_meta_path, index=False)\n","val_meta_df.to_csv(val_meta_path, index=False)\n","print(\"Saved aligned meta CSV:\")\n","print(\"   -\", train_meta_path)\n","print(\"   -\", val_meta_path)\n","\n","# ===== Save vocab =====\n","with open(os.path.join(OUT_DIR, 'vocab.txt'), 'w') as f:\n","    f.write(\",\".join(TOKENS))\n","print(\"Saved vocab.txt\")\n","\n","# ===== Quick preview =====\n","print(\"\\nPreview meta (val):\")\n","print(val_meta_df.head(5).to_string(index=False))\n","\n","print(\"\\n✅ DONE.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WkR5hb7fYFFA","executionInfo":{"status":"ok","timestamp":1756833502393,"user_tz":-60,"elapsed":108101,"user":{"displayName":"Peerasit Srisukontamit","userId":"15163058641388219245"}},"outputId":"09482d07-c3a8-43fd-ad41-5a07fb2f46d1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading raw CSV ...\n","✅ Loaded rows after filtering: 11,245,819\n","Building sequences (WINDOW_SIZE=4, NO_CONTEXT=False) ...\n"]},{"output_type":"stream","name":"stderr","text":["Sessions:  10%|▉         | 20000/201894 [00:37<05:43, 529.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["✅ Total sequences built: 1,070,621\n","Shuffling and splitting with aligned meta ...\n","Saved .npy to: /content/drive/MyDrive/Colab Notebooks/Flanker-GPT/human_dataset\n","train.npy: 963,558 | val.npy: 107,063\n","Saved aligned meta CSV:\n","   - /content/drive/MyDrive/Colab Notebooks/Flanker-GPT/human_dataset/train_meta.csv\n","   - /content/drive/MyDrive/Colab Notebooks/Flanker-GPT/human_dataset/val_meta.csv\n","Saved vocab.txt\n","\n","Preview meta (val):\n"," user_id  game_result_id  end_trial_num  context_used  trial4_layout trial4_target trial4_flanker  trial4_is_congruent trial4_response  trial4_correct  trial4_response_time                                                                                                                                                                                          trial_1_4_text  label_response_token_id\n","   11983      3435201562             35             1              6             U              D                    0               U               1                1007.0 0 0 D 0 0 0 0 D 0 0 0 0 L 0 0 0 0 D 0 0 0 0 D 0 0 0 0 0 0 0 0 0 0 0 0 D D L D D 0 0 0 0 0 0 0 0 0 0 L 0 0 0 L 0 0 0 0 0 0 L 0 L 0 0 0 0 0 0 0 0 D 0 0 0 0 D 0 0 0 0 0 0 0 D 0 U 0 D 0 0 0 0 0 0 0 D 0 0                        1\n","    2204      3513483047             25             1              3             R              R                    1               R               1                 927.0 0 0 0 0 0 0 0 0 0 0 L L L L L 0 0 0 0 0 0 0 0 0 0 R 0 0 0 R 0 0 0 0 0 0 R 0 R 0 0 0 0 0 0 0 0 R 0 0 0 0 0 0 R 0 0 R 0 0 R 0 0 0 0 0 0 R 0 0 0 0 0 0 R R 0 0 0 0 0 0 R 0 0 0 0 0 0 R 0 0 R 0 0 R 0 0 0 0                        0\n","   21394      3638508288              5             1              0             U              D                    0               U               1                 806.0 0 0 0 0 R 0 0 R 0 0 R 0 0 0 0 0 0 R 0 0 0 0 0 0 R 0 0 R 0 0 0 0 R 0 0 0 0 L 0 0 0 0 R 0 0 0 0 R 0 0 0 0 0 0 0 0 0 0 0 0 L L L L L 0 0 0 0 0 0 0 0 0 0 0 0 D 0 0 0 0 D 0 0 0 0 U 0 0 0 0 D 0 0 0 0 D 0 0                        1\n","   18861      2448956863             45             1              0             D              U                    0               D               1                1091.0 D 0 0 0 0 0 0 D 0 0 0 0 0 0 D 0 0 D 0 0 D 0 0 0 0 0 0 R 0 0 0 0 R 0 0 0 0 R 0 0 0 0 R 0 0 0 0 R 0 0 0 0 R 0 0 0 0 0 0 0 0 R 0 R 0 0 0 0 0 0 R 0 0 0 R 0 0 U 0 0 0 0 U 0 0 0 0 D 0 0 0 0 U 0 0 0 0 U 0 0                        3\n","   21701      4970745004             44             1              0             R              L                    0               R               1                 811.0 0 0 U 0 0 0 0 0 0 0 U 0 R 0 U 0 0 0 0 0 0 0 U 0 0 0 0 D 0 0 0 0 0 0 0 D 0 D 0 D 0 0 0 0 0 0 0 D 0 0 0 0 0 0 U 0 0 U 0 0 U 0 0 0 0 0 0 U 0 0 0 0 0 0 U 0 0 L 0 0 0 0 L 0 0 0 0 R 0 0 0 0 L 0 0 0 0 L 0 0                        0\n","\n","✅ DONE.\n"]}]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"lPB56EIxYHaW"}},{"cell_type":"code","source":["# ===== Imports =====\n","import os\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","import sys\n","\n","# Add model path\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/Flanker-GPT')\n","from model import GPT, GPTConfig\n","\n","# ===== Paths =====\n","data_dir = \"/content/drive/MyDrive/Colab Notebooks/Flanker-GPT/human_dataset\"\n","out_dir = \"/content/drive/MyDrive/Colab Notebooks/Flanker-GPT/checkpoints\"\n","os.makedirs(out_dir, exist_ok=True)\n","\n","# ===== Settings =====\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","batch_size = 64\n","block_size = 101  # 4 trials × 25 tokens + 1 label\n","max_iters = 2000\n","eval_interval = 200\n","learning_rate = 3e-4\n","vocab_size = 6  # ['R','U','L','D','0','=>']\n","\n","# ===== Load Dataset =====\n","train_data = np.load(os.path.join(data_dir, 'train.npy'))\n","val_data = np.load(os.path.join(data_dir, 'val.npy'))\n","\n","def get_batch(split):\n","    data = train_data if split == 'train' else val_data\n","    ix = np.random.randint(len(data), size=batch_size)\n","    x = torch.tensor(np.stack([d[:-1] for d in data[ix]]), dtype=torch.long, device=device)\n","    y = torch.tensor([d[-1] for d in data[ix]], dtype=torch.long, device=device)\n","    return x, y\n","\n","# ===== Create Model =====\n","config = GPTConfig(\n","    vocab_size=vocab_size,\n","    block_size=block_size,\n","    n_layer=4,\n","    n_head=4,\n","    n_embd=128\n",")\n","model = GPT(config).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","# ===== Eval Loss =====\n","@torch.no_grad()\n","def estimate_loss():\n","    model.eval()\n","    out = {}\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(10)\n","        for k in range(10):\n","            X, Y = get_batch(split)\n","            logits, _ = model(X)\n","            logits = logits[:, -1, :]  # use only last position\n","            loss = F.cross_entropy(logits, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","# ===== Train Loop =====\n","print(\"Training Human-Trained GPT-Flanker model...\")\n","for iter in range(max_iters):\n","    if iter % eval_interval == 0 or iter == max_iters - 1:\n","        losses = estimate_loss()\n","        print(f\"Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    xb, yb = get_batch('train')\n","    logits, _ = model(xb)\n","    logits = logits[:, -1, :]  # Predict only the final token\n","    loss = F.cross_entropy(logits, yb)\n","\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","# ===== Save Model =====\n","ckpt_path = os.path.join(out_dir, \"flanker_gpt_human.pt\")\n","torch.save(model.state_dict(), ckpt_path)\n","print(f\"✅ Model saved to {ckpt_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R2sA9hfSYIoc","executionInfo":{"status":"ok","timestamp":1756833650364,"user_tz":-60,"elapsed":135105,"user":{"displayName":"Peerasit Srisukontamit","userId":"15163058641388219245"}},"outputId":"0946853c-276d-40a1-dd0a-b4508d185573"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 0.79M\n","Training Human-Trained GPT-Flanker model...\n","Step 0: train loss 1.9709, val loss 1.9768\n","Step 200: train loss 0.2331, val loss 0.1650\n","Step 400: train loss 0.1259, val loss 0.1522\n","Step 600: train loss 0.1137, val loss 0.1261\n","Step 800: train loss 0.1660, val loss 0.1294\n","Step 1000: train loss 0.1737, val loss 0.1658\n","Step 1200: train loss 0.1393, val loss 0.1629\n","Step 1400: train loss 0.1241, val loss 0.1422\n","Step 1600: train loss 0.1312, val loss 0.1086\n","Step 1800: train loss 0.1041, val loss 0.2276\n","Step 1999: train loss 0.1474, val loss 0.0935\n","✅ Model saved to /content/drive/MyDrive/Colab Notebooks/Flanker-GPT/checkpoints/flanker_gpt_human.pt\n"]}]},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"J2B-82ILYI4o"}},{"cell_type":"code","source":["# ===== Imports =====\n","import os, sys, csv, math\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn.functional as F\n","\n","# Add model path\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/Flanker-GPT')\n","from model import GPT, GPTConfig\n","\n","# ===== Paths =====\n","base_dir   = \"/content/drive/MyDrive/Colab Notebooks/Flanker-GPT\"\n","data_dir   = f\"{base_dir}/human_dataset\"\n","ckpt_path  = f\"{base_dir}/checkpoints/flanker_gpt_human.pt\"\n","val_npy    = os.path.join(data_dir, \"val.npy\")\n","val_meta   = os.path.join(data_dir, \"val_meta.csv\")\n","vocab_txt  = os.path.join(data_dir, \"vocab.txt\")  # optional check\n","output_csv = f\"{base_dir}/gpt_human_val_predictions_stopping.csv\"\n","\n","# ===== Settings =====\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","TOKENS = ['R','U','L','D','0','=>']  # must match builder TOKENS\n","RESPONSE_TOKENS = ['R','U','L','D']  # restrict sampling to directions\n","stoi = {tok: i for i, tok in enumerate(TOKENS)}\n","itos = {i: tok for i, tok in enumerate(TOKENS)}\n","response_token_ids = [stoi[t] for t in RESPONSE_TOKENS]\n","\n","# ----- Stopping-Rule Params -----\n","delta = 3          # stop when (top_count - second_count) >= delta\n","max_samples = 100  # safety cap\n","restrict_to_response_tokens = True\n","\n","# ===== Load Model =====\n","# infer block_size from val.npy to support with/no-context automatically\n","val_data = np.load(val_npy)\n","block_size = val_data.shape[1] - 1\n","vocab_size = len(TOKENS)\n","\n","config = GPTConfig(\n","    vocab_size=vocab_size,\n","    block_size=block_size,\n","    n_layer=4,\n","    n_head=4,\n","    n_embd=128\n",")\n","model = GPT(config).to(device)\n","model.load_state_dict(torch.load(ckpt_path, map_location=device))\n","model.eval()\n","print(\"✅ Loaded Human-trained GPT model\")\n","print(f\"Loaded {len(val_data)} validation samples | block_size={block_size}\")\n","\n","# ===== Load aligned meta (for congruency & human RT) =====\n","assert os.path.exists(val_meta), f\"val_meta.csv not found at {val_meta}\"\n","meta = pd.read_csv(val_meta)\n","assert len(meta) == len(val_data), \"val_meta.csv must align with val.npy length\"\n","\n","# ===== Utils =====\n","def calculate_entropy(prob_dist_np):\n","    return -float(np.sum(prob_dist_np * np.log(prob_dist_np + 1e-12)))\n","\n","def sample_until_threshold(probs_full, delta=3, max_samples=100, restrict_ids=None):\n","    \"\"\"\n","    probs_full: 1D torch tensor over full vocab (sum=1)\n","    restrict_ids: list of token ids to sample from (renormalize over this subset)\n","    Returns: winner_id, k_used, stopped_bool, gap_at_stop\n","    \"\"\"\n","    if restrict_ids is not None:\n","        sub = probs_full[restrict_ids]\n","        sub = sub / sub.sum()\n","        id_map = restrict_ids\n","        def draw():\n","            idx_local = torch.multinomial(sub, num_samples=1, replacement=True).item()\n","            return id_map[idx_local]\n","    else:\n","        def draw():\n","            return torch.multinomial(probs_full, num_samples=1, replacement=True).item()\n","\n","    counts = {}\n","    for s in range(1, max_samples + 1):\n","        tok = draw()\n","        counts[tok] = counts.get(tok, 0) + 1\n","\n","        sorted_pairs = sorted(counts.items(), key=lambda kv: kv[1], reverse=True)\n","        top_count = sorted_pairs[0][1]\n","        second_count = sorted_pairs[1][1] if len(sorted_pairs) > 1 else 0\n","        gap = top_count - second_count\n","        if gap >= delta:\n","            winner_id = sorted_pairs[0][0]\n","            return winner_id, s, True, gap\n","\n","    # fallback: argmax over restricted or full distribution\n","    if restrict_ids is not None:\n","        sub = probs_full[restrict_ids]\n","        winner_local = torch.argmax(sub).item()\n","        winner_id = restrict_ids[winner_local]\n","    else:\n","        winner_id = torch.argmax(probs_full).item()\n","\n","    if len(counts) > 0:\n","        sorted_pairs = sorted(counts.items(), key=lambda kv: kv[1], reverse=True)\n","        top_count = sorted_pairs[0][1]\n","        second_count = sorted_pairs[1][1] if len(sorted_pairs) > 1 else 0\n","        gap = top_count - second_count\n","    else:\n","        gap = 0\n","    return winner_id, max_samples, False, gap\n","\n","# ===== Evaluate =====\n","records = []\n","correct = 0\n","\n","for i in range(len(val_data)):\n","    seq = torch.tensor(val_data[i][:-1], dtype=torch.long, device=device).unsqueeze(0)\n","    true_id = int(val_data[i][-1])\n","    true_tok = itos[true_id]\n","\n","    with torch.no_grad():\n","        logits, _ = model(seq)\n","        logits = logits[:, -1, :]  # only final position\n","        probs = F.softmax(logits, dim=-1).squeeze(0).cpu()\n","\n","        winner_id, k_used, stopped, gap_at_stop = sample_until_threshold(\n","            probs_full=probs,\n","            delta=delta,\n","            max_samples=max_samples,\n","            restrict_ids=response_token_ids if restrict_to_response_tokens else None\n","        )\n","        pred_tok = itos[winner_id]\n","\n","        is_correct = (winner_id == true_id)\n","        if is_correct:\n","            correct += 1\n","\n","        confidence = float(probs[winner_id].item())\n","        prob_true  = float(probs[true_id].item())\n","        entropy    = calculate_entropy(probs.numpy())\n","\n","    # meta from aligned CSV\n","    is_congruent = int(meta.loc[i, \"trial4_is_congruent\"])\n","    trial4_layout = int(meta.loc[i, \"trial4_layout\"])\n","    human_rt = float(meta.loc[i, \"trial4_response_time\"]) if \"trial4_response_time\" in meta.columns else np.nan\n","\n","    row = {\n","        \"example\": i + 1,\n","        \"true_response\": true_tok,\n","        \"predicted_response\": pred_tok,\n","        \"is_correct\": bool(is_correct),\n","\n","        \"confidence\": round(confidence, 6),\n","        \"prob_of_true_token\": round(prob_true, 6),\n","        \"entropy\": round(entropy, 6),\n","\n","        # stopping diagnostics\n","        \"k_samples\": int(k_used),\n","        \"stopped_by_delta\": bool(stopped),\n","        \"gap_at_stop\": int(gap_at_stop),\n","        \"delta\": int(delta),\n","        \"max_samples\": int(max_samples),\n","\n","        # meta\n","        \"is_congruent\": is_congruent,\n","        \"trial4_layout\": trial4_layout,\n","        \"human_rt_ms\": human_rt,\n","    }\n","\n","    # per-token probs (directions only)\n","    for tok in RESPONSE_TOKENS:\n","        row[f\"{tok}_prob\"] = round(float(probs[stoi[tok]].item()), 6)\n","\n","    records.append(row)\n","\n","# ===== Save Results =====\n","df = pd.DataFrame(records)\n","df.to_csv(output_csv, index=False)\n","acc = correct / len(val_data) * 100.0\n","print(\"\\n✅ Stopping-rule evaluation (human) complete.\")\n","print(f\"Accuracy = {acc:.2f}% (Δ={delta}, max={max_samples})\")\n","print(f\"Results saved to: {output_csv}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bha-8cLIYKUc","executionInfo":{"status":"ok","timestamp":1756834120515,"user_tz":-60,"elapsed":433688,"user":{"displayName":"Peerasit Srisukontamit","userId":"15163058641388219245"}},"outputId":"571e1778-7f1a-446e-d7c1-a3a21579a596"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 0.79M\n","✅ Loaded Human-trained GPT model\n","Loaded 107063 validation samples | block_size=101\n","\n","✅ Stopping-rule evaluation (human) complete.\n","Accuracy = 97.21% (Δ=3, max=100)\n","Results saved to: /content/drive/MyDrive/Colab Notebooks/Flanker-GPT/gpt_human_val_predictions_stopping.csv\n"]}]},{"cell_type":"markdown","source":["### Congruency Effect Analysis"],"metadata":{"id":"7Dq4ADpvYPkC"}},{"cell_type":"code","source":["# ===== Analyze Congruency Effect (No RT Version) =====\n","# This script evaluates accuracy, entropy, and k_samples by congruency.\n","# It assumes that your prediction CSV has: is_congruent, is_correct, entropy, confidence, k_samples\n","\n","import os, numpy as np, pandas as pd\n","\n","# ==== INPUT (update to the model you want to analyze) ====\n","base_dir   = \"/content/drive/MyDrive/Colab Notebooks/Flanker-GPT\"\n","pred_csv   = f\"{base_dir}/gpt_human_val_predictions_stopping.csv\"\n","\n","out_dir    = os.path.join(base_dir, \"congruency_effect\")\n","os.makedirs(out_dir, exist_ok=True)\n","\n","# ==== LOAD ====\n","df = pd.read_csv(pred_csv)\n","req = {\"is_congruent\",\"is_correct\",\"entropy\",\"confidence\",\"k_samples\"}\n","missing = list(req - set(df.columns))\n","assert not missing, f\"Missing columns: {missing}\"\n","\n","# ==== BASIC SUMMARIES ====\n","acc_overall = df[\"is_correct\"].mean()*100\n","acc_by_c = df.groupby(\"is_congruent\")[\"is_correct\"].mean().rename({0:\"incongruent\",1:\"congruent\"})*100\n","\n","ent_by_c = df.groupby(\"is_congruent\")[\"entropy\"].mean().rename({0:\"incongruent\",1:\"congruent\"})\n","k_by_c   = df.groupby(\"is_congruent\")[\"k_samples\"].mean().rename({0:\"incongruent\",1:\"congruent\"})\n","\n","print(\"=== Congruency Effect (Model, No RT) ===\")\n","print(f\"Accuracy overall: {acc_overall:.2f}%\")\n","print(f\"Accuracy by congruency (%):\\n{acc_by_c.to_string()}\")\n","print(f\"\\nEntropy mean by congruency:\\n{ent_by_c.to_string()}\")\n","print(f\"\\nk_samples mean by congruency:\\n{k_by_c.to_string()}\")\n","\n","# ==== SAVE TABLE ====\n","rows = []\n","rows.append({\"metric\":\"Accuracy_overall_%\", \"value\": acc_overall})\n","rows.append({\"metric\":\"Accuracy_congruent_%\", \"value\": acc_by_c.get(\"congruent\", np.nan)})\n","rows.append({\"metric\":\"Accuracy_incongruent_%\", \"value\": acc_by_c.get(\"incongruent\", np.nan)})\n","rows.append({\"metric\":\"Entropy_mean_congruent\", \"value\": ent_by_c.get(\"congruent\", np.nan)})\n","rows.append({\"metric\":\"Entropy_mean_incongruent\", \"value\": ent_by_c.get(\"incongruent\", np.nan)})\n","rows.append({\"metric\":\"k_mean_congruent\", \"value\": k_by_c.get(\"congruent\", np.nan)})\n","rows.append({\"metric\":\"k_mean_incongruent\", \"value\": k_by_c.get(\"incongruent\", np.nan)})\n","\n","out_csv = os.path.join(out_dir, os.path.basename(pred_csv).replace(\".csv\",\"_congruency_stats_NoRT.csv\"))\n","pd.DataFrame(rows).to_csv(out_csv, index=False)\n","\n","print(\"\\nSaved:\", out_csv)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oos8ceVeYRfO","executionInfo":{"status":"ok","timestamp":1756834553559,"user_tz":-60,"elapsed":397,"user":{"displayName":"Peerasit Srisukontamit","userId":"15163058641388219245"}},"outputId":"167161bd-1491-483e-a0ff-e9b171712e08"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["=== Congruency Effect (Model, No RT) ===\n","Accuracy overall: 97.21%\n","Accuracy by congruency (%):\n","is_congruent\n","incongruent    95.389022\n","congruent      99.015788\n","\n","Entropy mean by congruency:\n","is_congruent\n","incongruent    0.205579\n","congruent      0.090269\n","\n","k_samples mean by congruency:\n","is_congruent\n","incongruent    3.294631\n","congruent      3.081887\n","\n","Saved: /content/drive/MyDrive/Colab Notebooks/Flanker-GPT/congruency_effect/gpt_human_val_predictions_stopping_congruency_stats_NoRT.csv\n"]}]},{"cell_type":"markdown","source":["# WITHOUT CONTEXT"],"metadata":{"id":"UXtlsMlWQcEZ"}},{"cell_type":"markdown","source":["## Dataset Generation"],"metadata":{"id":"yA38gqvvQmLn"}},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pvi65d35QWjz","executionInfo":{"status":"ok","timestamp":1756834648368,"user_tz":-60,"elapsed":82871,"user":{"displayName":"Peerasit Srisukontamit","userId":"15163058641388219245"}},"outputId":"e389bf1d-b09b-4b39-8ae8-079606f1bc0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading raw CSV ...\n","✅ Loaded rows after filtering: 11,245,819\n","Building sequences (WINDOW_SIZE=4, NO_CONTEXT=True) ...\n"]},{"output_type":"stream","name":"stderr","text":["Sessions:  10%|▉         | 20000/201894 [00:37<05:36, 540.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Total sequences built: 1,070,621\n","Shuffling and splitting with aligned meta ...\n","Saved .npy to: /content/drive/MyDrive/Colab Notebooks/Flanker-GPT/human_dataset_noctx\n","train.npy: 963,558 | val.npy: 107,063\n","Saved aligned meta CSV:\n","   - /content/drive/MyDrive/Colab Notebooks/Flanker-GPT/human_dataset_noctx/train_meta.csv\n","   - /content/drive/MyDrive/Colab Notebooks/Flanker-GPT/human_dataset_noctx/val_meta.csv\n","Saved vocab.txt\n","\n","🔎 Preview meta (val):\n"," user_id  game_result_id  end_trial_num  context_used  trial4_layout trial4_target trial4_flanker  trial4_is_congruent trial4_response  trial4_correct  trial4_response_time                                    trial_1_4_text  label_response_token_id\n","   11983      3435201562             35             0              6             U              D                    0               U               1                1007.0 0 0 D 0 0 0 0 0 0 0 D 0 U 0 D 0 0 0 0 0 0 0 D 0 0                        1\n","    2204      3513483047             25             0              3             R              R                    1               R               1                 927.0 R 0 0 0 0 0 0 R 0 0 0 0 0 0 R 0 0 R 0 0 R 0 0 0 0                        0\n","   21394      3638508288              5             0              0             U              D                    0               U               1                 806.0 0 0 D 0 0 0 0 D 0 0 0 0 U 0 0 0 0 D 0 0 0 0 D 0 0                        1\n","   18861      2448956863             45             0              0             D              U                    0               D               1                1091.0 0 0 U 0 0 0 0 U 0 0 0 0 D 0 0 0 0 U 0 0 0 0 U 0 0                        3\n","   21701      4970745004             44             0              0             R              L                    0               R               1                 811.0 0 0 L 0 0 0 0 L 0 0 0 0 R 0 0 0 0 L 0 0 0 0 L 0 0                        0\n","\n","✅ DONE.\n"]}],"source":["# ===== Imports =====\n","import os\n","import math\n","import random\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","# ===== Paths =====\n","INPUT_CSV = \"/content/drive/MyDrive/Colab Notebooks/Flanker-GPT/trialdata74.csv\"\n","OUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/Flanker-GPT/human_dataset_noctx\"\n","os.makedirs(OUT_DIR, exist_ok=True)\n","\n","# ===== Config =====\n","RNG_SEED = 42 # reproducibility\n","VAL_RATIO = 0.10 # 10% validation\n","WINDOW_SIZE = 4 # seq length for context window\n","NO_CONTEXT = True # True -> trial 4 only (no-context baseline) #*\n","MAX_SESSIONS = 20000 # limit number of sessions for speed; set None for all\n","KEEP_RT_COL = True # export human RT into meta for later calibration\n","DROP_NAN_RT_META = False # if True, drop rows with NaN RT from meta (sequence still saved)\n","\n","# ===== Tokens (4 directions + '0' + '=>') =====\n","TOKENS = ['R', 'U', 'L', 'D', '0', '=>']  # NOTE: label is response_direction ∈ {R,U,L,D}\n","stoi = {tok: i for i, tok in enumerate(TOKENS)}\n","itos = {i: tok for tok, i in stoi.items()}\n","\n","# ===== Layout mapping (layout_id → 5 positions in 5x5) =====\n","LAYOUTS = {\n","    0: [  2,  7, 12, 17, 22 ],  # Vertical\n","    1: [ 10, 11, 12, 13, 14 ],  # Horizontal\n","    2: [  4,  7, 10, 17, 24 ],  # Left hook\n","    3: [  0,  7, 14, 17, 20 ],  # Right hook\n","    4: [ 11, 13,  2, 20, 24 ],  # Top hook\n","    5: [  0,  4, 22, 11, 13 ],  # Bottom hook\n","    6: [ 10,  2, 12, 14, 22 ],  # Surround\n","}\n","\n","# ===== Helpers =====\n","def build_matrix(layout_id, target_dir, flanker_dir):\n","    \"\"\"Build a 5x5 matrix (flattened to length 25) for one trial.\"\"\"\n","    mat = ['0'] * 25\n","    pos = LAYOUTS[int(layout_id)]\n","    # pos[2] is target; others are flankers\n","    mat[pos[0]] = flanker_dir\n","    mat[pos[1]] = flanker_dir\n","    mat[pos[2]] = target_dir\n","    mat[pos[3]] = flanker_dir\n","    mat[pos[4]] = flanker_dir\n","    return mat\n","\n","def encode_sequence(trials_tokens, label_token):\n","    \"\"\"Flatten trials (each 25 tokens) + '=>' + label into token ids.\"\"\"\n","    seq = []\n","    for t in trials_tokens:\n","        seq.extend(t)\n","    seq.append('=>')\n","    seq.append(label_token)\n","    return [stoi[t] for t in seq]\n","\n","def is_congruent_row(target_dir, flanker_dir):\n","    return str(target_dir) == str(flanker_dir)\n","\n","def valid_dir(tok):\n","    return str(tok) in {'R','U','L','D'}\n","\n","# ===== Load RAW CSV =====\n","dtype_map = {\n","    'game_result_id': 'int64',\n","    'trial_num': 'int64',\n","    'target_direction': 'string',\n","    'flanker_direction': 'string',\n","    'stimulus_layout': 'int64',\n","    'response_direction': 'string',\n","    'correct': 'string',\n","    'response_time': 'float64',\n","    'user_id': 'int64',\n","}\n","print(\"Loading raw CSV ...\")\n","df = pd.read_csv(INPUT_CSV, dtype=dtype_map)\n","\n","# keep only complete direction rows\n","df = df[ df['target_direction'].apply(valid_dir)\n","       & df['flanker_direction'].apply(valid_dir)\n","       & df['response_direction'].apply(valid_dir) ].copy()\n","\n","# sort by user/session/trial\n","df = df.sort_values(by=['user_id', 'game_result_id', 'trial_num']).reset_index(drop=True)\n","print(f\"✅ Loaded rows after filtering: {len(df):,}\")\n","\n","# ===== Build sequences =====\n","random.seed(RNG_SEED)\n","np.random.seed(RNG_SEED)\n","\n","encoded_sequences = []\n","meta_rows = []\n","\n","grouped = df.groupby(['game_result_id'], sort=False)\n","total_sessions = df['game_result_id'].nunique()\n","limit_sessions = min(MAX_SESSIONS, total_sessions) if MAX_SESSIONS is not None else total_sessions\n","\n","print(f\"Building sequences (WINDOW_SIZE={WINDOW_SIZE}, NO_CONTEXT={NO_CONTEXT}) ...\")\n","for i, (gid, g) in enumerate(tqdm(grouped, total=total_sessions, desc=\"Sessions\")):\n","    if i >= limit_sessions:\n","        break\n","\n","    rows = g.to_dict('records')\n","    if len(rows) < WINDOW_SIZE:\n","        continue\n","\n","    for end_idx in range(WINDOW_SIZE-1, len(rows)):\n","        start_idx = end_idx - (WINDOW_SIZE - 1)\n","        window_rows = rows[start_idx:end_idx+1]\n","        use_rows = [window_rows[-1]] if NO_CONTEXT else window_rows\n","\n","        trials_tokens = []\n","        skip = False\n","        for r in use_rows:\n","            try:\n","                mat = build_matrix(r['stimulus_layout'], r['target_direction'], r['flanker_direction'])\n","            except Exception as e:\n","                skip = True\n","                break\n","            trials_tokens.append(mat)\n","        if skip:\n","            continue\n","\n","        # label is the *human response* on trial 4\n","        label_token = str(window_rows[-1]['response_direction'])\n","        token_ids = encode_sequence(trials_tokens, label_token)\n","        encoded_sequences.append(token_ids)\n","\n","        # meta for trial 4 (end of window)\n","        t4 = window_rows[-1]\n","        rt_val = float(t4['response_time']) if pd.notna(t4['response_time']) else math.nan\n","\n","        meta_rows.append({\n","            \"user_id\":             int(t4['user_id']),\n","            \"game_result_id\":      int(t4['game_result_id']),\n","            \"end_trial_num\":       int(t4['trial_num']),\n","            \"context_used\":        int(not NO_CONTEXT),\n","            \"trial4_layout\":       int(t4['stimulus_layout']),\n","            \"trial4_target\":       str(t4['target_direction']),\n","            \"trial4_flanker\":      str(t4['flanker_direction']),\n","            \"trial4_is_congruent\": int(is_congruent_row(t4['target_direction'], t4['flanker_direction'])),\n","            \"trial4_response\":     str(t4['response_direction']),\n","            \"trial4_correct\":      1 if str(t4['correct']).upper() == 'T' else 0,\n","            \"trial4_response_time\": rt_val if KEEP_RT_COL else np.nan,\n","            # raw text (optional, helpful for debugging; keep or drop as needed)\n","            \"trial_1_4_text\": \" \".join([tok for trial in trials_tokens for tok in trial]),\n","            \"label_response_token_id\": int(stoi[label_token]),\n","        })\n","\n","print(f\"Total sequences built: {len(encoded_sequences):,}\")\n","\n","# ===== Sanity: length alignment =====\n","assert len(encoded_sequences) == len(meta_rows), \"encoded_sequences and meta_rows length mismatch!\"\n","\n","# ===== Shuffle & Split with aligned meta =====\n","print(\"Shuffling and splitting with aligned meta ...\")\n","N = len(encoded_sequences)\n","perm = np.random.RandomState(RNG_SEED).permutation(N)\n","encoded_sequences = [encoded_sequences[i] for i in perm]\n","meta_rows        = [meta_rows[i]        for i in perm]\n","\n","split_idx = int(N * (1 - VAL_RATIO))\n","train_set = encoded_sequences[:split_idx]\n","val_set   = encoded_sequences[split_idx:]\n","\n","train_meta = meta_rows[:split_idx]\n","val_meta   = meta_rows[split_idx:]\n","\n","# (Optional) drop rows with NaN RT from meta ONLY (sequence remains; usually keep them)\n","if DROP_NAN_RT_META:\n","    def _drop_nan_rt(meta_list):\n","        kept = []\n","        for m in meta_list:\n","            rt = m.get(\"trial4_response_time\", np.nan)\n","            if not (isinstance(rt, float) and math.isnan(rt)):\n","                kept.append(m)\n","        return kept\n","    train_meta = _drop_nan_rt(train_meta)\n","    val_meta   = _drop_nan_rt(val_meta)\n","\n","# ===== Save .npy =====\n","np.save(os.path.join(OUT_DIR, 'train.npy'), np.array(train_set, dtype=np.int32))\n","np.save(os.path.join(OUT_DIR, 'val.npy'),   np.array(val_set,   dtype=np.int32))\n","print(f\"Saved .npy to: {OUT_DIR}\")\n","print(f\"train.npy: {len(train_set):,} | val.npy: {len(val_set):,}\")\n","\n","# ===== Save aligned meta CSV =====\n","train_meta_df = pd.DataFrame(train_meta)\n","val_meta_df   = pd.DataFrame(val_meta)\n","\n","train_meta_path = os.path.join(OUT_DIR, 'train_meta.csv')\n","val_meta_path   = os.path.join(OUT_DIR, 'val_meta.csv')\n","train_meta_df.to_csv(train_meta_path, index=False)\n","val_meta_df.to_csv(val_meta_path, index=False)\n","print(\"Saved aligned meta CSV:\")\n","print(\"   -\", train_meta_path)\n","print(\"   -\", val_meta_path)\n","\n","# ===== Save vocab =====\n","with open(os.path.join(OUT_DIR, 'vocab.txt'), 'w') as f:\n","    f.write(\",\".join(TOKENS))\n","print(\"Saved vocab.txt\")\n","\n","# ===== Quick preview =====\n","print(\"\\n🔎 Preview meta (val):\")\n","print(val_meta_df.head(5).to_string(index=False))\n","\n","print(\"\\n✅ DONE.\")"]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"ChXU-_xITHBR"}},{"cell_type":"code","source":["# ===== Imports =====\n","import os\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","import sys\n","\n","# Add model path\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/Flanker-GPT')\n","from model import GPT, GPTConfig\n","\n","# ===== Paths =====\n","data_dir = \"/content/drive/MyDrive/Colab Notebooks/Flanker-GPT/human_dataset_noctx\" #*\n","out_dir = \"/content/drive/MyDrive/Colab Notebooks/Flanker-GPT/checkpoints\"\n","os.makedirs(out_dir, exist_ok=True)\n","\n","# ===== Settings =====\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","batch_size = 64\n","block_size = 26  # 1 trial × 25 tokens + 1 label #*\n","max_iters = 2000\n","eval_interval = 200\n","learning_rate = 3e-4\n","vocab_size = 6  # ['R','U','L','D','0','=>']\n","\n","# ===== Load Dataset =====\n","train_data = np.load(os.path.join(data_dir, 'train.npy'))\n","val_data = np.load(os.path.join(data_dir, 'val.npy'))\n","\n","def get_batch(split):\n","    data = train_data if split == 'train' else val_data\n","    ix = np.random.randint(len(data), size=batch_size)\n","    x = torch.tensor(np.stack([d[:-1] for d in data[ix]]), dtype=torch.long, device=device)\n","    y = torch.tensor([d[-1] for d in data[ix]], dtype=torch.long, device=device)\n","    return x, y\n","\n","# ===== Create Model =====\n","config = GPTConfig(\n","    vocab_size=vocab_size,\n","    block_size=block_size,\n","    n_layer=4,\n","    n_head=4,\n","    n_embd=128\n",")\n","model = GPT(config).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","# ===== Eval Loss =====\n","@torch.no_grad()\n","def estimate_loss():\n","    model.eval()\n","    out = {}\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(10)\n","        for k in range(10):\n","            X, Y = get_batch(split)\n","            logits, _ = model(X)\n","            logits = logits[:, -1, :]\n","            loss = F.cross_entropy(logits, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","# ===== Train Loop =====\n","print(\"Training Human-Trained GPT-Flanker model...\")\n","for iter in range(max_iters):\n","    if iter % eval_interval == 0 or iter == max_iters - 1:\n","        losses = estimate_loss()\n","        print(f\"Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    xb, yb = get_batch('train')\n","    logits, _ = model(xb)\n","    logits = logits[:, -1, :]  # Predict only the final token\n","    loss = F.cross_entropy(logits, yb)\n","\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","# ===== Save Model =====\n","ckpt_path = os.path.join(out_dir, \"flanker_gpt_human_noctx.pt\") #*\n","torch.save(model.state_dict(), ckpt_path)\n","print(f\"✅ Model saved to {ckpt_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fSm9qef8TIWl","executionInfo":{"status":"ok","timestamp":1756834779277,"user_tz":-60,"elapsed":68828,"user":{"displayName":"Peerasit Srisukontamit","userId":"15163058641388219245"}},"outputId":"3673b419-a8c5-4afa-c629-048b6047223a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 0.79M\n","Training Human-Trained GPT-Flanker model...\n","Step 0: train loss 2.0304, val loss 2.0159\n","Step 200: train loss 0.2285, val loss 0.1568\n","Step 400: train loss 0.1241, val loss 0.1472\n","Step 600: train loss 0.1048, val loss 0.1171\n","Step 800: train loss 0.1609, val loss 0.1270\n","Step 1000: train loss 0.1733, val loss 0.1594\n","Step 1200: train loss 0.1313, val loss 0.1502\n","Step 1400: train loss 0.1269, val loss 0.1468\n","Step 1600: train loss 0.1320, val loss 0.1061\n","Step 1800: train loss 0.1073, val loss 0.2277\n","Step 1999: train loss 0.1471, val loss 0.0945\n","✅ Model saved to /content/drive/MyDrive/Colab Notebooks/Flanker-GPT/checkpoints/flanker_gpt_human_noctx.pt\n"]}]},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"XI-bk7UWTzpM"}},{"cell_type":"code","source":["# ===== Imports =====\n","import os, sys, csv, math\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn.functional as F\n","\n","# Add model path\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/Flanker-GPT')\n","from model import GPT, GPTConfig\n","\n","# ===== Paths =====\n","base_dir   = \"/content/drive/MyDrive/Colab Notebooks/Flanker-GPT\"\n","data_dir   = f\"{base_dir}/human_dataset_noctx\"            #*\n","ckpt_path  = f\"{base_dir}/checkpoints/flanker_gpt_human_noctx.pt\"  #*\n","val_npy    = os.path.join(data_dir, \"val.npy\")\n","val_meta   = os.path.join(data_dir, \"val_meta.csv\")       #*\n","output_csv = f\"{base_dir}/gpt_human_noctx_val_predictions_stopping.csv\"  #*\n","\n","\n","# ===== Settings =====\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","TOKENS = ['R','U','L','D','0','=>']  # must match builder TOKENS\n","RESPONSE_TOKENS = ['R','U','L','D']  # restrict sampling to directions\n","stoi = {tok: i for i, tok in enumerate(TOKENS)}\n","itos = {i: tok for i, tok in enumerate(TOKENS)}\n","response_token_ids = [stoi[t] for t in RESPONSE_TOKENS]\n","\n","# ----- Stopping-Rule Params -----\n","delta = 3          # stop when (top_count - second_count) >= delta\n","max_samples = 100  # safety cap\n","restrict_to_response_tokens = True\n","\n","# ===== Load Model =====\n","# infer block_size from val.npy to support with/no-context automatically\n","val_data = np.load(val_npy)\n","block_size = val_data.shape[1] - 1\n","vocab_size = len(TOKENS)\n","\n","config = GPTConfig(\n","    vocab_size=vocab_size,\n","    block_size=block_size,\n","    n_layer=4,\n","    n_head=4,\n","    n_embd=128\n",")\n","model = GPT(config).to(device)\n","model.load_state_dict(torch.load(ckpt_path, map_location=device))\n","model.eval()\n","print(\"✅ Loaded Human-trained GPT model\")\n","print(f\"Loaded {len(val_data)} validation samples | block_size={block_size}\")\n","\n","# ===== Load aligned meta (for congruency & human RT) =====\n","assert os.path.exists(val_meta), f\"val_meta.csv not found at {val_meta}\"\n","meta = pd.read_csv(val_meta)\n","assert len(meta) == len(val_data), \"val_meta.csv must align with val.npy length\"\n","\n","# ===== Utils =====\n","def calculate_entropy(prob_dist_np):\n","    return -float(np.sum(prob_dist_np * np.log(prob_dist_np + 1e-12)))\n","\n","def sample_until_threshold(probs_full, delta=3, max_samples=100, restrict_ids=None):\n","    \"\"\"\n","    probs_full: 1D torch tensor over full vocab (sum=1)\n","    restrict_ids: list of token ids to sample from (renormalize over this subset)\n","    Returns: winner_id, k_used, stopped_bool, gap_at_stop\n","    \"\"\"\n","    if restrict_ids is not None:\n","        sub = probs_full[restrict_ids]\n","        sub = sub / sub.sum()\n","        id_map = restrict_ids\n","        def draw():\n","            idx_local = torch.multinomial(sub, num_samples=1, replacement=True).item()\n","            return id_map[idx_local]\n","    else:\n","        def draw():\n","            return torch.multinomial(probs_full, num_samples=1, replacement=True).item()\n","\n","    counts = {}\n","    for s in range(1, max_samples + 1):\n","        tok = draw()\n","        counts[tok] = counts.get(tok, 0) + 1\n","\n","        sorted_pairs = sorted(counts.items(), key=lambda kv: kv[1], reverse=True)\n","        top_count = sorted_pairs[0][1]\n","        second_count = sorted_pairs[1][1] if len(sorted_pairs) > 1 else 0\n","        gap = top_count - second_count\n","        if gap >= delta:\n","            winner_id = sorted_pairs[0][0]\n","            return winner_id, s, True, gap\n","\n","    # fallback: argmax over restricted or full distribution\n","    if restrict_ids is not None:\n","        sub = probs_full[restrict_ids]\n","        winner_local = torch.argmax(sub).item()\n","        winner_id = restrict_ids[winner_local]\n","    else:\n","        winner_id = torch.argmax(probs_full).item()\n","\n","    if len(counts) > 0:\n","        sorted_pairs = sorted(counts.items(), key=lambda kv: kv[1], reverse=True)\n","        top_count = sorted_pairs[0][1]\n","        second_count = sorted_pairs[1][1] if len(sorted_pairs) > 1 else 0\n","        gap = top_count - second_count\n","    else:\n","        gap = 0\n","    return winner_id, max_samples, False, gap\n","\n","# ===== Evaluate =====\n","records = []\n","correct = 0\n","\n","for i in range(len(val_data)):\n","    seq = torch.tensor(val_data[i][:-1], dtype=torch.long, device=device).unsqueeze(0)\n","    true_id = int(val_data[i][-1])\n","    true_tok = itos[true_id]\n","\n","    with torch.no_grad():\n","        logits, _ = model(seq)\n","        logits = logits[:, -1, :]  # only final position\n","        probs = F.softmax(logits, dim=-1).squeeze(0).cpu()\n","\n","        winner_id, k_used, stopped, gap_at_stop = sample_until_threshold(\n","            probs_full=probs,\n","            delta=delta,\n","            max_samples=max_samples,\n","            restrict_ids=response_token_ids if restrict_to_response_tokens else None\n","        )\n","        pred_tok = itos[winner_id]\n","\n","        is_correct = (winner_id == true_id)\n","        if is_correct:\n","            correct += 1\n","\n","        confidence = float(probs[winner_id].item())\n","        prob_true  = float(probs[true_id].item())\n","        entropy    = calculate_entropy(probs.numpy())\n","\n","    # meta from aligned CSV\n","    is_congruent = int(meta.loc[i, \"trial4_is_congruent\"])\n","    trial4_layout = int(meta.loc[i, \"trial4_layout\"])\n","    human_rt = float(meta.loc[i, \"trial4_response_time\"]) if \"trial4_response_time\" in meta.columns else np.nan\n","\n","    row = {\n","        \"example\": i + 1,\n","        \"true_response\": true_tok,\n","        \"predicted_response\": pred_tok,\n","        \"is_correct\": bool(is_correct),\n","\n","        \"confidence\": round(confidence, 6),\n","        \"prob_of_true_token\": round(prob_true, 6),\n","        \"entropy\": round(entropy, 6),\n","\n","        # stopping diagnostics\n","        \"k_samples\": int(k_used),\n","        \"stopped_by_delta\": bool(stopped),\n","        \"gap_at_stop\": int(gap_at_stop),\n","        \"delta\": int(delta),\n","        \"max_samples\": int(max_samples),\n","\n","        # meta\n","        \"is_congruent\": is_congruent,\n","        \"trial4_layout\": trial4_layout,\n","        \"human_rt_ms\": human_rt,\n","    }\n","\n","    # per-token probs (directions only)\n","    for tok in RESPONSE_TOKENS:\n","        row[f\"{tok}_prob\"] = round(float(probs[stoi[tok]].item()), 6)\n","\n","    records.append(row)\n","\n","# ===== Save Results =====\n","df = pd.DataFrame(records)\n","df.to_csv(output_csv, index=False)\n","acc = correct / len(val_data) * 100.0\n","print(\"\\n✅ Stopping-rule evaluation (human) complete.\")\n","print(f\"Accuracy = {acc:.2f}% (Δ={delta}, max={max_samples})\")\n","print(f\"Results saved to: {output_csv}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OMrDI0kKT1am","executionInfo":{"status":"ok","timestamp":1756835175139,"user_tz":-60,"elapsed":378484,"user":{"displayName":"Peerasit Srisukontamit","userId":"15163058641388219245"}},"outputId":"f742c0e1-a640-40a9-c293-fecc1f8d3dcf"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 0.79M\n","✅ Loaded Human-trained GPT model\n","Loaded 107063 validation samples | block_size=26\n","\n","✅ Stopping-rule evaluation (human) complete.\n","Accuracy = 97.21% (Δ=3, max=100)\n","Results saved to: /content/drive/MyDrive/Colab Notebooks/Flanker-GPT/gpt_human_noctx_val_predictions_stopping.csv\n"]}]},{"cell_type":"markdown","source":["### Congruency Effect Analysis"],"metadata":{"id":"NFuVz6KPVej6"}},{"cell_type":"code","source":["# ===== Analyze Congruency Effect (No RT Version) =====\n","# This script evaluates accuracy, entropy, and k_samples by congruency.\n","# It assumes that your prediction CSV has: is_congruent, is_correct, entropy, confidence, k_samples\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, numpy as np, pandas as pd\n","\n","# ==== INPUT (update to the model you want to analyze) ====\n","base_dir   = \"/content/drive/MyDrive/Colab Notebooks/Flanker-GPT\"\n","pred_csv   = f\"{base_dir}/gpt_human_noctx_val_predictions_stopping.csv\" #*\n","\n","out_dir    = os.path.join(base_dir, \"congruency_effect\")\n","os.makedirs(out_dir, exist_ok=True)\n","\n","# ==== LOAD ====\n","df = pd.read_csv(pred_csv)\n","req = {\"is_congruent\",\"is_correct\",\"entropy\",\"confidence\",\"k_samples\"}\n","missing = list(req - set(df.columns))\n","assert not missing, f\"Missing columns: {missing}\"\n","\n","# ==== BASIC SUMMARIES ====\n","acc_overall = df[\"is_correct\"].mean()*100\n","acc_by_c = df.groupby(\"is_congruent\")[\"is_correct\"].mean().rename({0:\"incongruent\",1:\"congruent\"})*100\n","\n","ent_by_c = df.groupby(\"is_congruent\")[\"entropy\"].mean().rename({0:\"incongruent\",1:\"congruent\"})\n","k_by_c   = df.groupby(\"is_congruent\")[\"k_samples\"].mean().rename({0:\"incongruent\",1:\"congruent\"})\n","\n","print(\"=== Congruency Effect (Model, No RT) ===\")\n","print(f\"Accuracy overall: {acc_overall:.2f}%\")\n","print(f\"Accuracy by congruency (%):\\n{acc_by_c.to_string()}\")\n","print(f\"\\nEntropy mean by congruency:\\n{ent_by_c.to_string()}\")\n","print(f\"\\nk_samples mean by congruency:\\n{k_by_c.to_string()}\")\n","\n","# ==== SAVE TABLE ====\n","rows = []\n","rows.append({\"metric\":\"Accuracy_overall_%\", \"value\": acc_overall})\n","rows.append({\"metric\":\"Accuracy_congruent_%\", \"value\": acc_by_c.get(\"congruent\", np.nan)})\n","rows.append({\"metric\":\"Accuracy_incongruent_%\", \"value\": acc_by_c.get(\"incongruent\", np.nan)})\n","rows.append({\"metric\":\"Entropy_mean_congruent\", \"value\": ent_by_c.get(\"congruent\", np.nan)})\n","rows.append({\"metric\":\"Entropy_mean_incongruent\", \"value\": ent_by_c.get(\"incongruent\", np.nan)})\n","rows.append({\"metric\":\"k_mean_congruent\", \"value\": k_by_c.get(\"congruent\", np.nan)})\n","rows.append({\"metric\":\"k_mean_incongruent\", \"value\": k_by_c.get(\"incongruent\", np.nan)})\n","\n","out_csv = os.path.join(out_dir, os.path.basename(pred_csv).replace(\".csv\",\"_congruency_stats_NoRT.csv\"))\n","pd.DataFrame(rows).to_csv(out_csv, index=False)\n","\n","print(\"\\nSaved:\", out_csv)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qZQDDT0IVhYo","executionInfo":{"status":"ok","timestamp":1756835213916,"user_tz":-60,"elapsed":1408,"user":{"displayName":"Peerasit Srisukontamit","userId":"15163058641388219245"}},"outputId":"c71a441f-892b-48c8-cdc8-84aacec84824"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","=== Congruency Effect (Model, No RT) ===\n","Accuracy overall: 97.21%\n","Accuracy by congruency (%):\n","is_congruent\n","incongruent    95.398383\n","congruent      99.015788\n","\n","Entropy mean by congruency:\n","is_congruent\n","incongruent    0.218666\n","congruent      0.080407\n","\n","k_samples mean by congruency:\n","is_congruent\n","incongruent    3.315318\n","congruent      3.070554\n","\n","Saved: /content/drive/MyDrive/Colab Notebooks/Flanker-GPT/congruency_effect/gpt_human_noctx_val_predictions_stopping_congruency_stats_NoRT.csv\n"]}]},{"cell_type":"markdown","source":["# HUMAN BASELINE"],"metadata":{"id":"uZYI4QDI4HdI"}},{"cell_type":"markdown","source":["### Human RT Analysis"],"metadata":{"id":"auwLZtho4KpZ"}},{"cell_type":"code","source":["# ===== Human RT Analysis (distribution + stats, patched) =====\n","\n","import os, math\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# ===== Paths =====\n","base_dir = \"/content/drive/MyDrive/Colab Notebooks/Flanker-GPT/human_dataset_noctx\"\n","train_meta_path = os.path.join(base_dir, \"train_meta.csv\")\n","val_meta_path   = os.path.join(base_dir, \"val_meta.csv\")\n","out_dir         = os.path.join(base_dir, \"rt_analysis\")\n","os.makedirs(out_dir, exist_ok=True)\n","\n","# ===== Load =====\n","train_meta = pd.read_csv(train_meta_path)\n","val_meta   = pd.read_csv(val_meta_path)\n","df = pd.concat([train_meta, val_meta], ignore_index=True)\n","\n","# ===== Check required columns =====\n","required_cols = [\"trial4_is_congruent\",\"trial4_response_time\",\"trial4_correct\"]\n","missing = [c for c in required_cols if c not in df.columns]\n","assert not missing, f\"Missing columns in meta: {missing}\"\n","\n","# Force integer dtype (avoid weird mixed dtypes)\n","df[\"trial4_is_congruent\"] = df[\"trial4_is_congruent\"].astype(int)\n","df[\"trial4_correct\"]      = df[\"trial4_correct\"].astype(int)\n","\n","# ===== Detect RT units & clean =====\n","rt_raw = df[\"trial4_response_time\"].astype(float)\n","\n","# Heuristic: if median < 10, assume seconds → convert to ms\n","if rt_raw.median(skipna=True) < 10:\n","    df[\"human_rt_ms\"] = rt_raw * 1000.0\n","else:\n","    df[\"human_rt_ms\"] = rt_raw\n","\n","# Drop NaN values\n","df = df.dropna(subset=[\"human_rt_ms\"]).copy()\n","\n","# Trim outliers with a simple cutoff\n","RT_MIN, RT_MAX = 150, 3000  # ms\n","df[\"rt_keep\"] = (df[\"human_rt_ms\"] >= RT_MIN) & (df[\"human_rt_ms\"] <= RT_MAX)\n","kept_ratio = df[\"rt_keep\"].mean()\n","df = df[df[\"rt_keep\"]].copy()\n","\n","# ===== Helper functions =====\n","def rt_summary(x: pd.Series):\n","    x = x.dropna()\n","    q = x.quantile([0,.1,.25,.5,.75,.9,.95,.99])\n","    return {\n","        \"n\":   int(x.count()),\n","        \"mean\": float(x.mean()),\n","        \"std\":  float(x.std(ddof=1)),\n","        \"min\":  float(x.min()),\n","        \"p10\":  float(q.loc[0.10]),\n","        \"p25\":  float(q.loc[0.25]),\n","        \"p50\":  float(q.loc[0.50]),\n","        \"p75\":  float(q.loc[0.75]),\n","        \"p90\":  float(q.loc[0.90]),\n","        \"p95\":  float(q.loc[0.95]),\n","        \"p99\":  float(q.loc[0.99]),\n","        \"max\":  float(x.max()),\n","    }\n","\n","def cohens_d(a, b):\n","    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n","    if len(a) < 2 or len(b) < 2: return np.nan\n","    m1, m2 = a.mean(), b.mean()\n","    s1, s2 = a.std(ddof=1), b.std(ddof=1)\n","    sp = np.sqrt(((len(a)-1)*s1*s1 + (len(b)-1)*s2*s2) / (len(a)+len(b)-2))\n","    return (m1 - m2)/sp if sp > 0 else np.nan\n","\n","# ===== Overall summary =====\n","overall = rt_summary(df[\"human_rt_ms\"])\n","\n","# ===== By congruency =====\n","cong_map = {0: \"incongruent\", 1: \"congruent\"}\n","by_cong_df = df.groupby(\"trial4_is_congruent\")[\"human_rt_ms\"] \\\n","               .apply(lambda s: pd.Series(rt_summary(s))) \\\n","               .reset_index()\n","by_cong_named = {\n","    cong_map[int(row[\"trial4_is_congruent\"])]: row.drop(\"trial4_is_congruent\").to_dict()\n","    for _, row in by_cong_df.iterrows()\n","}\n","\n","# ===== By correctness =====\n","corr_map = {0: \"incorrect\", 1: \"correct\"}\n","by_corr_df = df.groupby(\"trial4_correct\")[\"human_rt_ms\"] \\\n","               .apply(lambda s: pd.Series(rt_summary(s))) \\\n","               .reset_index()\n","by_correct_named = {\n","    corr_map[int(row[\"trial4_correct\"])]: row.drop(\"trial4_correct\").to_dict()\n","    for _, row in by_corr_df.iterrows()\n","}\n","\n","# ===== Congruency gap & effect size =====\n","rt_inc = df.loc[df[\"trial4_is_congruent\"]==0, \"human_rt_ms\"].values\n","rt_con = df.loc[df[\"trial4_is_congruent\"]==1, \"human_rt_ms\"].values\n","gap_ms = float(np.mean(rt_inc) - np.mean(rt_con))\n","d_eff  = float(cohens_d(rt_inc, rt_con))\n","\n","# ===== Save stats table =====\n","stats_rows = [{\"group\":\"overall\", **overall}]\n","for k, v in by_cong_named.items():\n","    stats_rows.append({\"group\": f\"congruency:{k}\", **v})\n","for k, v in by_correct_named.items():\n","    stats_rows.append({\"group\": f\"correctness:{k}\", **v})\n","\n","stats_df = pd.DataFrame(stats_rows)\n","stats_csv = os.path.join(out_dir, \"human_rt_stats.csv\")\n","stats_df.to_csv(stats_csv, index=False)\n","\n","# ===== Plots =====\n","plt.figure(figsize=(6,4))\n","plt.hist(df[\"human_rt_ms\"], bins=60)\n","plt.xlabel(\"RT (ms)\"); plt.ylabel(\"Count\"); plt.title(\"Human RT Histogram (trimmed)\")\n","plt.tight_layout(); plt.savefig(os.path.join(out_dir,\"rt_hist_overall.png\"), dpi=160); plt.close()\n","\n","# hist by congruency (overlay)\n","plt.figure(figsize=(6,4))\n","plt.hist(rt_con, bins=60, alpha=0.6, label=\"congruent\")\n","plt.hist(rt_inc, bins=60, alpha=0.6, label=\"incongruent\")\n","plt.xlabel(\"RT (ms)\"); plt.ylabel(\"Count\"); plt.title(\"RT by Congruency (Histogram)\")\n","plt.legend(); plt.tight_layout()\n","plt.savefig(os.path.join(out_dir,\"rt_hist_by_congruency.png\"), dpi=160); plt.close()\n","\n","# CDF by congruency\n","def ecdf(x):\n","    xs = np.sort(x); ys = np.arange(1, len(xs)+1)/len(xs); return xs, ys\n","x1,y1 = ecdf(rt_con); x2,y2 = ecdf(rt_inc)\n","plt.figure(figsize=(6,4))\n","plt.plot(x1,y1,label=\"congruent\")\n","plt.plot(x2,y2,label=\"incongruent\")\n","plt.xlabel(\"RT (ms)\"); plt.ylabel(\"CDF\"); plt.title(\"RT CDF by Congruency\")\n","plt.legend(); plt.tight_layout()\n","plt.savefig(os.path.join(out_dir,\"rt_cdf_by_congruency.png\"), dpi=160); plt.close()\n","\n","# boxplot\n","plt.figure(figsize=(5,4))\n","plt.boxplot([rt_con, rt_inc], labels=[\"congruent\",\"incongruent\"])\n","plt.ylabel(\"RT (ms)\"); plt.title(\"RT Boxplot by Congruency\")\n","plt.tight_layout(); plt.savefig(os.path.join(out_dir,\"rt_box_by_congruency.png\"), dpi=160); plt.close()\n","\n","# ===== Print quick summary =====\n","print(\"====== Human RT (trimmed) ======\")\n","print(f\"Kept ratio after trimming [{RT_MIN},{RT_MAX}] ms: {kept_ratio*100:.1f}%\")\n","print(\"Overall:\", overall)\n","print(\"By congruency:\", by_cong_named)\n","print(\"By correctness:\", by_correct_named)\n","print(f\"Congruency gap (inc - con): {gap_ms:.2f} ms  |  Cohen's d: {d_eff:.3f}\")\n","print(f\"\\nSaved stats to: {stats_csv}\")\n","print(f\"Plots saved in: {out_dir}\")\n","print(by_cong_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EmM0rDyk2dzA","executionInfo":{"status":"ok","timestamp":1756835242486,"user_tz":-60,"elapsed":5597,"user":{"displayName":"Peerasit Srisukontamit","userId":"15163058641388219245"}},"outputId":"621f5957-9380-48a7-e1f3-57ebd521ff30"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-676542640.py:141: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n","  plt.boxplot([rt_con, rt_inc], labels=[\"congruent\",\"incongruent\"])\n"]},{"output_type":"stream","name":"stdout","text":["====== Human RT (trimmed) ======\n","Kept ratio after trimming [150,3000] ms: 100.0%\n","Overall: {'n': 1070284, 'mean': 788.3175502950619, 'std': 204.9591513113322, 'min': 200.0, 'p10': 592.0, 'p25': 657.0, 'p50': 746.0, 'p75': 865.0, 'p90': 1027.0, 'p95': 1158.0, 'p99': 1527.0, 'max': 3000.0}\n","By congruency: {'incongruent': {'level_1': 'max', 'human_rt_ms': 3000.0}, 'congruent': {'level_1': 'max', 'human_rt_ms': 2992.0}}\n","By correctness: {'incorrect': {'level_1': 'max', 'human_rt_ms': 2984.0}, 'correct': {'level_1': 'max', 'human_rt_ms': 3000.0}}\n","Congruency gap (inc - con): 75.84 ms  |  Cohen's d: 0.377\n","\n","Saved stats to: /content/drive/MyDrive/Colab Notebooks/Flanker-GPT/human_dataset_noctx/rt_analysis/human_rt_stats.csv\n","Plots saved in: /content/drive/MyDrive/Colab Notebooks/Flanker-GPT/human_dataset_noctx/rt_analysis\n","    trial4_is_congruent level_1    human_rt_ms\n","0                     0       n  533637.000000\n","1                     0    mean     826.343261\n","2                     0     std     211.115736\n","3                     0     min     200.000000\n","4                     0     p10     622.000000\n","5                     0     p25     691.000000\n","6                     0     p50     784.000000\n","7                     0     p75     910.000000\n","8                     0     p90    1071.000000\n","9                     0     p95    1204.000000\n","10                    0     p99    1581.000000\n","11                    0     max    3000.000000\n","12                    1       n  536647.000000\n","13                    1    mean     750.505122\n","14                    1     std     191.294197\n","15                    1     min     200.000000\n","16                    1     p10     575.000000\n","17                    1     p25     632.000000\n","18                    1     p50     710.000000\n","19                    1     p75     814.000000\n","20                    1     p90     967.000000\n","21                    1     p95    1095.000000\n","22                    1     p99    1454.000000\n","23                    1     max    2992.000000\n"]}]},{"cell_type":"markdown","source":["### Congruency Effect Analysis"],"metadata":{"id":"i47Bhx6o4PAd"}},{"cell_type":"code","source":["# ===== Accuracy statistics =====\n","acc_overall = df[\"trial4_correct\"].mean()\n","acc_by_cong = df.groupby(\"trial4_is_congruent\")[\"trial4_correct\"].mean().rename({0:\"incongruent\",1:\"congruent\"})\n","\n","# Accuracy vs RT deciles + by congruency (simple CAF)\n","df[\"rt_decile\"] = pd.qcut(df[\"human_rt_ms\"], 10, labels=False, duplicates=\"drop\")\n","acc_by_rtdecile = df.groupby(\"rt_decile\")[\"trial4_correct\"].mean()\n","acc_by_rtdecile_cong = df.groupby([\"trial4_is_congruent\",\"rt_decile\"])[\"trial4_correct\"].mean().unstack(0).rename(columns={0:\"incongruent\",1:\"congruent\"})\n","\n","# Save accuracy summary\n","acc_df = pd.DataFrame({\n","    \"metric\": [\"overall_accuracy\"],\n","    \"value\": [acc_overall]\n","})\n","acc_by_cong_df = acc_by_cong.reset_index().rename(columns={\"trial4_is_congruent\":\"congruency\",\"trial4_correct\":\"accuracy\"})\n","acc_by_cong_df[\"congruency\"] = acc_by_cong_df[\"congruency\"].map({0:\"incongruent\",1:\"congruent\"})\n","acc_out = os.path.join(out_dir, \"human_accuracy_stats.csv\")\n","pd.concat([acc_df, acc_by_cong_df], ignore_index=True).to_csv(acc_out, index=False)\n","\n","# Plot: accuracy vs RT deciles\n","plt.figure(figsize=(6,4))\n","plt.plot(acc_by_rtdecile.index, acc_by_rtdecile.values, marker=\"o\")\n","plt.xlabel(\"RT decile (slow → fast)\"); plt.ylabel(\"Accuracy\")\n","plt.title(\"Accuracy vs RT deciles\")\n","plt.tight_layout(); plt.savefig(os.path.join(out_dir,\"acc_vs_rt_deciles.png\"), dpi=160); plt.close()\n","\n","# Plot: Conditional Accuracy Function (CAF) by congruency\n","plt.figure(figsize=(6,4))\n","for col in [\"congruent\",\"incongruent\"]:\n","    if col in acc_by_rtdecile_cong.columns:\n","        plt.plot(acc_by_rtdecile_cong.index, acc_by_rtdecile_cong[col].values, marker=\"o\", label=col)\n","plt.xlabel(\"RT decile\"); plt.ylabel(\"Accuracy\")\n","plt.title(\"Conditional Accuracy Function (CAF)\")\n","plt.legend(); plt.tight_layout()\n","plt.savefig(os.path.join(out_dir,\"caf_by_congruency.png\"), dpi=160); plt.close()\n","\n","print(\"\\n====== Accuracy ======\")\n","print(f\"Overall accuracy: {acc_overall*100:.2f}%\")\n","print(\"By congruency (%):\")\n","print((acc_by_cong*100).rename_axis(\"congruency\").to_string())\n","print(f\"\\nSaved accuracy stats to: {acc_out}\")"],"metadata":{"id":"QsdIDMP64TND","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756835268083,"user_tz":-60,"elapsed":520,"user":{"displayName":"Peerasit Srisukontamit","userId":"15163058641388219245"}},"outputId":"66997193-ffbd-464a-c95e-dc34d1212914"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","====== Accuracy ======\n","Overall accuracy: 97.15%\n","By congruency (%):\n","congruency\n","incongruent    95.451402\n","congruent      98.829584\n","\n","Saved accuracy stats to: /content/drive/MyDrive/Colab Notebooks/Flanker-GPT/human_dataset_noctx/rt_analysis/human_accuracy_stats.csv\n"]}]}]}